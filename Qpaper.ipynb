{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varnikabaskar/data-science/blob/main/Qpaper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART-1"
      ],
      "metadata": {
        "id": "b9KwjjT8VjSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)DataScience and BigData"
      ],
      "metadata": {
        "id": "f5H6xHdFslHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data. Key components of data science include:\n",
        "* Data Cleaning and Preparation\n",
        "* Exploratory Data Analysis (EDA)\n",
        "* Feature Engineering\n",
        "* Machine Learning and Predictive Modeling\n",
        "* Data Visualization\n",
        "* Communication of Results"
      ],
      "metadata": {
        "id": "kibfiQdis0So",
        "outputId": "a7de991b-3238-4557-df0f-2553912f1f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ac2662b65a5f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Data science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data. Key components of data science include:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Big data refers to extremely large and complex data sets that traditional data processing methods are inadequate to handle efficiently. It is characterized by the three :\n",
        "* Volume\n",
        "* Velocity\n",
        "* Variety."
      ],
      "metadata": {
        "id": "YryYeQmjt-iV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "b305e7c3-73be-48d7-b46b-f76d20801374"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-96d3152be091>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Big data refers to extremely large and complex data sets that traditional data processing methods are inadequate to handle efficiently. It is characterized by the three :\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Common Errors In Retrieving Data"
      ],
      "metadata": {
        "id": "zQKfFFcFuQSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "* Missing Data\n",
        "* Duplicate Data\n",
        "* Incorrect Data Types\n",
        "* Outliers\n",
        "* Data Integrity Issues\n",
        "* Data Inconsistencies"
      ],
      "metadata": {
        "id": "W_h4aDxWueQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)Classify The List of Data Into Their Types"
      ],
      "metadata": {
        "id": "f-KkErQ7vPFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical variable-Ethnic group,academic major,sexual preference,gender\n",
        "Numerical variable-Age\n",
        "Numerical (Discrete)-Family Size\n",
        "Numerical (Continuous)-IQ score,networth,temperature\n",
        "Ordinal-Third place finish"
      ],
      "metadata": {
        "id": "hs6WdSKSvhMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)Differentiate Discrete and Continuous Variables"
      ],
      "metadata": {
        "id": "VAAYTBWRyMs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DISCRETE-Discrete variables are variables that can only take on specific, distinct values. These values are often counted in whole numbers and have gaps or interruptions between them.\n",
        "ex-* Number of students in a class (can only be whole numbers like 25, 26, etc.).\n",
        "   * Number of cars in a parking lot."
      ],
      "metadata": {
        "id": "NBNX6ILIynSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONTINUOUS-Continuous variables are variables that can take on any value within a specified range. They can have an infinite number of possible values and can be measured with great precision.\n",
        "ex-* Height of individuals"
      ],
      "metadata": {
        "id": "t1d09Wjyy46z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)Percentile Rank and Example"
      ],
      "metadata": {
        "id": "1tnPRPNfzHNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Percentile rank is a measure that indicates the percentage of scores that fall below or are equal to a particular value in a dataset.\n",
        "It is a way to express the relative standing of a specific score within the entire distribution.\n",
        "The percentile rank is often expressed as a percentage and ranges from 0% to 100%."
      ],
      "metadata": {
        "id": "31lKYLIxzN1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6)Helen sent 10 greeting cards to her friends and she received back 8 cards,what kind of relationship it is?"
      ],
      "metadata": {
        "id": "7Y0t25M-zjii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The relationship described between Helen sending greeting cards to her friends and receiving cards back is an example of a reciprocal relationship.\n",
        " In a reciprocal relationship, there is a mutual exchange or interaction between two parties where actions are returned in kind."
      ],
      "metadata": {
        "id": "aGCV2oc_1UTw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "87c3397f-3fb2-4474-d7b0-766a87de95a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-1bef79435f52>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    The relationship described between Helen sending greeting cards to her friends and receiving cards back is an example of a reciprocal relationship.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7)Attributes of a Numpy Array and Example"
      ],
      "metadata": {
        "id": "-6Z7Mn1l3gSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Here are some key attributes of a NumPy array:\n",
        "* Shape\n",
        "* Datatype\n",
        "* Size\n",
        "* No of Dimension\n",
        "* Strides"
      ],
      "metadata": {
        "id": "F3VhPYeF38ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "my_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"Shape:\", my_array.shape)\n",
        "print(\"Data Type:\", my_array.dtype)\n",
        "print(\"Size:\", my_array.size)\n",
        "print(\"Number of Dimensions:\", my_array.ndim)\n",
        "print(\"Strides:\", my_array.strides)"
      ],
      "metadata": {
        "id": "5R0NjaM14iId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8)Create a Dataframe with key and data pair and find the sum of each key and also display the result"
      ],
      "metadata": {
        "id": "SAlcWAFH4wsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_1={'Key':['A','B','A','C','B','C'],\n",
        "      'Data':[10,20,40,5,10,10]}\n",
        "df=pd.DataFrame(data_1)\n",
        "print(df)\n",
        "key_sums = df.groupby('Key')['Data'].sum().reset_index()\n",
        "print(\"\\nSum of each key:\")\n",
        "print(key_sums)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mGoF3z753Fj",
        "outputId": "3b30678d-591b-45f4-90c4-e27ba87e60cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Key  Data\n",
            "0   A    10\n",
            "1   B    20\n",
            "2   A    40\n",
            "3   C     5\n",
            "4   B    10\n",
            "5   C    10\n",
            "\n",
            "Sum of each key:\n",
            "  Key  Data\n",
            "0   A    50\n",
            "1   B    30\n",
            "2   C    15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9)Purpose of errorbar function in Matplotlib and give Example"
      ],
      "metadata": {
        "id": "uJVmI4ic7Fm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The errorbar function in Matplotlib is used to create error bar plots.\n",
        "Error bars are graphical representations of the uncertainty or variability of a set of data.\n",
        "They are typically used to visually indicate the confidence intervals, standard deviations, or other statistical measures associated with each data point."
      ],
      "metadata": {
        "id": "s_qiAXW1_28L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "5b37f11e-e27c-4d3b-bdda-7ac13db38c0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-7b6e50e12f83>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    The errorbar function in Matplotlib is used to create error bar plots.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [2, 4, 6, 8, 10]\n",
        "y_err = [0.5, 0.3, 0.7, 0.2, 0.4]\n",
        "plt.errorbar(x, y, yerr=y_err, fmt='o-', label='Data with Error Bars')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.title('Error Bar Example')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v36TRCCUAKHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10)Showcase 3-Dimensional Drawing in Matplotlib with corresponding Python code"
      ],
      "metadata": {
        "id": "9AMstBnOAZ1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "\n",
        "theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)\n",
        "z = np.linspace(-2, 2, 100)\n",
        "r = z**2 + 1\n",
        "x = r * np.sin(theta)\n",
        "y = r * np.cos(theta)\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot(x, y, z, label='3D Spiral')\n",
        "ax.set_xlabel('X-axis')\n",
        "ax.set_ylabel('Y-axis')\n",
        "ax.set_zlabel('Z-axis')\n",
        "ax.set_title('3D Spiral Plot')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FyPQ5hT8Axix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART-**B**"
      ],
      "metadata": {
        "id": "Tucgz-ZSMdRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11)a:Examine the different facets of data with the challenges in their processing"
      ],
      "metadata": {
        "id": "hIauY7tnM64f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "->Volume:\n",
        "Challenge: The sheer volume of data generated daily, especially with the rise of big data, can overwhelm traditional processing systems.\n",
        "Solution: Implementing scalable and distributed computing systems like Hadoop or Spark to handle large datasets.\n",
        "\n",
        "\n",
        "->Velocity:\n",
        "Challenge: Real-time data streams from sources such as social media or IoT devices require fast processing to derive timely insights.\n",
        "Solution: Stream processing frameworks like Apache Kafka or Apache Flink can handle and analyze data in real-time.\n",
        "\n",
        "->Variety:\n",
        "Challenge: Data comes in various formats – structured, semi-structured, and unstructured – making it challenging to integrate and process cohesively.\n",
        "Solution: Flexible data processing tools and techniques, such as NoSQL databases and schema-on-read approaches, can handle diverse data types.\n",
        "\n",
        "-->Veracity:\n",
        "Challenge: Data quality and accuracy can be compromised due to errors, missing values, or inconsistencies.\n",
        "Solution: Implementing data cleaning, validation, and quality assurance processes to ensure the accuracy and reliability of the data.\n",
        "\n",
        "->Value:\n",
        "Challenge: Extracting meaningful insights from data can be challenging, especially when dealing with a large volume of irrelevant or redundant information.\n",
        "Solution: Advanced analytics and machine learning algorithms can help identify patterns, trends, and outliers, enabling the extraction of valuable insights.\n",
        "\n",
        "->Security:\n",
        "Challenge: Ensuring the confidentiality, integrity, and availability of data is crucial, especially with the increasing frequency of cyber threats.\n",
        "Solution: Implementing robust security measures, including encryption, access controls, and regular audits, to safeguard sensitive information.\n",
        "\n",
        "->Interoperability:\n",
        "Challenge: Integrating data from diverse sources with different formats and standards can be challenging.\n",
        "Solution: Using standardized data formats and protocols, and employing data integration tools can facilitate interoperability.\n",
        "\n",
        "->Scalability:\n",
        "Challenge: Ensuring that data processing systems can scale horizontally to handle increased workloads.\n",
        "Solution: Designing systems with scalability in mind, utilizing cloud computing resources, and employing distributed computing architectures."
      ],
      "metadata": {
        "id": "JJCGbehvMk-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11)b:Explore the various steps associated in datascience process and explain any 3 with Example"
      ],
      "metadata": {
        "id": "IjNme5XeN0tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "->Problem Formulation:\n",
        "Explanation: This initial step involves understanding the problem at hand, defining goals, and establishing the scope of the project.\n",
        "It requires collaboration between data scientists and domain experts to identify key business questions that can be addressed through data analysis.\n",
        "\n",
        "->Data Collection and Cleaning:\n",
        "Explanation: In this stage, relevant data is gathered from various sources. This may involve accessing databases, APIs, or collecting data through surveys.\n",
        "Once collected, the data needs to be cleaned and preprocessed to handle missing values, outliers, and inconsistencies. Cleaning ensures that the data is in a suitable format for analysis.\n",
        "\n",
        "->Exploratory Data Analysis (EDA):\n",
        "Explanation: EDA involves exploring and understanding the characteristics of the data.\n",
        "This includes statistical summaries, visualizations, and identifying patterns or trends.\n",
        "EDA helps data scientists gain insights into the underlying structure of the data, which can inform subsequent modeling decisions.\n",
        "\n",
        "->Feature Engineering:\n",
        "Explanation: Feature engineering involves creating new variables (features) from the existing data to enhance the performance of machine learning models.\n",
        " It aims to capture relevant information and improve the model's ability to generalize to new, unseen data.\n",
        "\n",
        " ->Model Development:\n",
        "Explanation: This step involves selecting and training a model based on the goals of the project.\n",
        "The choice of model depends on the nature of the problem (classification, regression, clustering) and the characteristics of the data. Multiple models may be tested to find the one that performs best.\n",
        "\n",
        "->Model Evaluation:\n",
        "Explanation: The performance of the model is assessed using evaluation metrics relevant to the problem.\n",
        "This step helps ensure that the model generalizes well to new, unseen data and meets the defined objectives."
      ],
      "metadata": {
        "id": "mHk9w8JZOJ5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  +---------------------+         +-------------------------+         +-------------------------+\n",
        "  | Problem Formulation |         | Data Collection &       |         | Exploratory Data        |\n",
        "  |                     |         | Cleaning                |         | Analysis (EDA)          |\n",
        "  +----------+----------+         +----------+--------------+         +----------+--------------+\n",
        "             |                                |                                |\n",
        "             v                                v                                v\n",
        "  +-------------------------+   +-------------------------+   +-------------------------+\n",
        "  |    Feature Engineering  |   |   Model Development      |   |   Model Evaluation        |\n",
        "  +-------------------------+   +-------------------------+   +-------------------------+\n",
        "                                        |\n",
        "                                        v\n",
        "                              +-------------------------+\n",
        "                              | Model Deployment        |\n",
        "                              +-------------------------+"
      ],
      "metadata": {
        "id": "xvsLtF4QPmIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12)a:Demonstrate the different types of variables used in data analysis with an Example for each"
      ],
      "metadata": {
        "id": "nQZdvvz2PnRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. Categorical Variables:\n",
        "Categorical variables represent categories or labels and can take on a limited, fixed number of values. They are often divided into nominal and ordinal categories.\n",
        "i)Nominal Variable:\n",
        "Definition: Nominal variables represent categories without any inherent order.\n",
        "Example: Colors (e.g., red, blue, green)\n",
        "ii)Ordinal Variable:\n",
        "Definition: Ordinal variables have categories with a meaningful order or rank.\n",
        "Example: Education level (e.g., high school, bachelor's, master's, Ph.D.)\n",
        "\n",
        "2. Numerical Variables:\n",
        "Numerical variables represent measurable quantities and can be either discrete or continuous.\n",
        "\n",
        "*Discrete Variable:\n",
        "Definition: Discrete variables take on distinct, separate values, often integers.\n",
        "Example: Number of cars in a parking lot (1, 2, 3, ...)\n",
        "\n",
        "*Continuous Variable:\n",
        "Definition: Continuous variables can take on an infinite number of values within a given range.\n",
        "Example: Height of individuals (can be any value within a range, like 165.5 cm)"
      ],
      "metadata": {
        "id": "FnProTdwP6Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data={\"Nominal\":['red','blue','green'],\n",
        "      \"Ordinal\":['High School',' Bachelors',' Masters'],\n",
        "      \"Discrete\":[0,1,2],\n",
        "      \"Continuous\":[165.2,170.2,188.1]}\n",
        "df=pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "np0LH90qQyBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12)b:The no of friends reported by FaceBook users is Summarized in the following Frequency Distribution"
      ],
      "metadata": {
        "id": "5KzBgig1SJ-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HISTOGRAM"
      ],
      "metadata": {
        "id": "txcr9fNuhmVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "73jWdZ4sLwFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "friends_range = [\"400-above\", \"350-399\", \"300-349\", \"250-299\", \"200-249\", \"150-199\", \"100-149\", \"50-99\", \"0-49\"]\n",
        "frequency = [2, 5, 12, 17, 23, 49, 27, 29, 36]"
      ],
      "metadata": {
        "id": "18507-PuS5MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(friends_range, frequency, color='blue')\n",
        "plt.xlabel('Number of Friends')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Facebook Friends Frequency Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JVAEkXjGLrna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAPE OF THE DISTRIBUTION"
      ],
      "metadata": {
        "id": "MBXjaLBzhrGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis"
      ],
      "metadata": {
        "id": "2nI5kWXKMTOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "skewness = skew(frequency)\n",
        "\n",
        "print(\"Skewness of the distribution:\", skewness)"
      ],
      "metadata": {
        "id": "-txqVlTDiPik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERCENTILE RANK OF THE DATA"
      ],
      "metadata": {
        "id": "gDXijxqYhu_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interval_index = friends_range.index(\"300-349\")\n",
        "cumulative_frequency = sum(frequency[:interval_index + 1])\n",
        "total_observations = sum(frequency)\n",
        "percentile_rank = (cumulative_frequency / total_observations) * 100\n",
        "print(f\"Approximate Percentile Rank for the interval 300-350: {percentile_rank:.2f}%\")"
      ],
      "metadata": {
        "id": "xLjp3JYNNkp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RELATIVE FREQUENCY"
      ],
      "metadata": {
        "id": "7NwAo7nFh19L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_observations = sum(frequency)\n",
        "relative_frequencies = [freq / total_observations for freq in frequency]\n",
        "for i in range(len(friends_range)):\n",
        "    print(f\"Relative Frequency for {friends_range[i]}: {relative_frequencies[i]:.4f}\")"
      ],
      "metadata": {
        "id": "B-qVrFTZNyhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEM AND LEAF DISPLAY"
      ],
      "metadata": {
        "id": "91htsS_Zh52V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvehIEI1iXuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13)a-i)Categorize the different types of relationships using scatter plots"
      ],
      "metadata": {
        "id": "1vzjNNYrOw7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Scatter plots are useful for visualizing relationships between two continuous variables.\n",
        "The nature of the relationship in a scatter plot can help identify various types of relationships between variables.\n",
        "Here are several types of relationships that can be observed in scatter plots:"
      ],
      "metadata": {
        "id": "AzJE36zzOt7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.arange(1, 11)\n",
        "y = 2 * x + np.random.normal(0, 2, 10)\n",
        "plt.scatter(x, y)\n",
        "plt.title(\"Positive Linear Relationship\")\n",
        "plt.xlabel(\"X-axis\")\n",
        "plt.ylabel(\"Y-axis\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KBeOaAkDPMYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = -2 * x + np.random.normal(0, 2, 10)\n",
        "plt.scatter(x, y)\n",
        "plt.title(\"Negative Linear Relationship\")\n",
        "plt.xlabel(\"X-axis\")\n",
        "plt.ylabel(\"Y-axis\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VrEqKN5pPcVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.concatenate([np.random.normal(5, 1, 5), np.random.normal(15, 1, 5)])\n",
        "y = np.concatenate([np.random.normal(10, 2, 5), np.random.normal(20, 2, 5)])\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.title(\"Clustered Relationship\")\n",
        "plt.xlabel(\"X-axis\")\n",
        "plt.ylabel(\"Y-axis\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EnklvDIFPljE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13)a-ii)Each of the following pairs represent the number of licensed drivers(X)and the number of cars (Y) for seven houses in my neighborhood"
      ],
      "metadata": {
        "id": "DrDzQ-hQZsVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCATTERPLOT"
      ],
      "metadata": {
        "id": "vwMTJD9febCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "drivers = [5, 5, 2, 2, 3, 1, 2]\n",
        "cars = [4, 3, 2, 2, 2, 1, 2]\n",
        "# Scatterplot\n",
        "plt.scatter(drivers, cars)\n",
        "plt.title('Scatterplot of Licensed Drivers vs. Number of Cars')\n",
        "plt.xlabel('Licensed Drivers (X)')\n",
        "plt.ylabel('Number of Cars (Y)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxC4e1a3aWC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEAST SQUARE EQUATION"
      ],
      "metadata": {
        "id": "fBhDAqa4ehgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(drivers)\n",
        "sum_x = sum(drivers)\n",
        "sum_y = sum(cars)\n",
        "sum_xy = sum(x * y for x, y in zip(drivers, cars))\n",
        "sum_x_squared = sum(x**2 for x in drivers)\n",
        "\n",
        "b1 = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x**2)\n",
        "b0 = (sum_y - b1 * sum_x) / n\n",
        "# Least squares equation\n",
        "equation = f\"Y = {b0:.2f} + {b1:.2f}X\"\n",
        "print(\"Least Squares Equation:\", equation)"
      ],
      "metadata": {
        "id": "b_6P2bQtemv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ESTIMATION OF STANDARD ERROR"
      ],
      "metadata": {
        "id": "_ltdPhr1e277"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_y = [b0 + b1 * x for x in drivers]\n",
        "se = np.sqrt(sum((y - y_hat)**2 for y, y_hat in zip(cars, predicted_y)) / (n - 2))\n",
        "print(\"Standard Error of Estimate (Sy/x):\", se)"
      ],
      "metadata": {
        "id": "6GMt41eAe8Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13)b-i)In studies dating back over 100years ,its well established  that regression  toward the mean occurs between the heights of fathers and the heights  of their adult sons"
      ],
      "metadata": {
        "id": "um3otcmlhIhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Regression toward the mean is a statistical phenomenon where extreme values tend to move toward the average (mean) upon subsequent measurements. In the context of the heights of fathers and sons, if a father is exceptionally tall or short, the son's height is likely to be closer to the average height for all sons.\n",
        "\n",
        "Option 6 correctly captures this idea by stating that fathers of short sons (who are on the extreme lower end) will tend to be taller than their sons but shorter than the mean for all fathers. This aligns with the concept of regression toward the mean, where extreme values (short sons) are expected to move toward the average (mean for all fathers).\n",
        "\n",
        "Options 1, 2, 3, 4, and 5 do not accurately represent the concept of regression toward the mean in this context."
      ],
      "metadata": {
        "id": "c8sCp0BaoBl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13)b-ii) Interpret the value of r2in correlation based analysis"
      ],
      "metadata": {
        "id": "PMB75KiFfLE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Here are interpretations of different values of R^2:\n",
        "->R^2=0:\n",
        "Interpretation: None of the variability in the dependent variable (Y) is explained by the independent variable(s) (X).\n",
        "Implication: The model does not provide any meaningful information or prediction.\n",
        "\n",
        "->0<R^2<1:\n",
        "Interpretation: A proportion of the variability in the dependent variable is explained by the independent variable(s).\n",
        "Implication: The model has some predictive power, but there is still unexplained variability.\n",
        "\n",
        "->R^2=1:\n",
        "Interpretation: The dependent variable (Y) is perfectly predictable from the independent variable(s) (X).\n",
        "Implication: The model provides a perfect fit to the data.\n",
        "\n",
        "->R^2<0:\n",
        "Interpretation: This is not a valid scenario.should always be between 0 and 1.\n",
        "Implication: Something may be wrong with the analysis, or the model may not be appropriate for the data."
      ],
      "metadata": {
        "id": "ia5iye-igGzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate R-squared\n",
        "r2 = r2_score(y, y_pred)\n",
        "plt.scatter(X, y, label='Actual data')\n",
        "plt.plot(X, y_pred, color='red', label='Regression line')\n",
        "plt.title(f'Linear Regression with R-squared: {r2:.3f}')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f'R-squared: {r2:.3f}')"
      ],
      "metadata": {
        "id": "fTFDn9TpfjI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. a)Imagine you have a series of data that represents the amount of precipitation each day for a year in a given city.load the daily rainfall statistics for the city of chennai in 2021 which is given in a csv file chennairainfall2021.csv using pandas generate a histogram for rainy days,and find out that have high rainfall"
      ],
      "metadata": {
        "id": "Al1YaYGqjF5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas matplotlib"
      ],
      "metadata": {
        "id": "cgkuoAOhkABR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "file_path = 'chennairainfall2021.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "rainy_days = df[df['Rainfall'] > 0]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(rainy_days['Rainfall'], bins=20, color='blue', edgecolor='black')\n",
        "plt.title('Rainy Days Histogram - Chennai 2021')\n",
        "plt.xlabel('Rainfall (mm)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "high_rainfall_days = df[df['Rainfall'] > 50]\n",
        "print(\"Days with high rainfall (>50mm):\")\n",
        "print(high_rainfall_days[['Date', 'Rainfall']])"
      ],
      "metadata": {
        "id": "gfu1hhYkkMfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.b) Consider that an E-commerce organization like Amazon,have different regions sales on northsales,southsales,westsales,eastsales.csv files.they want to combine north and west region sales and south and east sales to find the aggregate sales of these collabrating regions help them to do so using python code"
      ],
      "metadata": {
        "id": "oZwjWYopUKhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "id": "pPz1m9W_UCgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "north_sales = pd.read_csv('northsales.csv')\n",
        "south_sales = pd.read_csv('southsales.csv')\n",
        "west_sales = pd.read_csv('westsales.csv')\n",
        "east_sales = pd.read_csv('eastsales.csv')\n",
        "# Combine north and west region sales\n",
        "north_west_sales = pd.concat([north_sales, west_sales], ignore_index=True)\n",
        "# Combine south and east region sales\n",
        "south_east_sales = pd.concat([south_sales, east_sales], ignore_index=True)\n",
        "aggregate_north_west_sales = north_west_sales['Sales'].sum()\n",
        "aggregate_south_east_sales = south_east_sales['Sales'].sum()\n",
        "\n",
        "print(\"Aggregate Sales for North and West regions:\", aggregate_north_west_sales)\n",
        "print(\"Aggregate Sales for South and East regions:\", aggregate_south_east_sales)"
      ],
      "metadata": {
        "id": "yabhJJINWNON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15)a:How text and image annotations are done using Python?give an example ofyour own with appropiate python code"
      ],
      "metadata": {
        "id": "xg24hjdqWbwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "data = [1, 3, 7, 5, 2, 4, 6]\n",
        "ax.plot(data)\n",
        "ax.text(2, 5, 'Max Value', fontsize=12, color='red', ha='center', va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QL7WRVC7Xt8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('example_image.jpg')\n",
        "cv2.rectangle(image, (100, 50), (300, 200), (0, 255, 0), 2)\n",
        "cv2.putText(image, 'Object', (120, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "cv2.imshow('Annotated Image', image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "s9xtDKg-X_pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15)b:Appraise the following i)Histograms ii)Binnings iii)Density with appropriate python code"
      ],
      "metadata": {
        "id": "05ji6oEDYNfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HISTOGRAMS"
      ],
      "metadata": {
        "id": "25yUZZ7YZJE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A histogram is a graphical representation of the distribution of a dataset. Matplotlib provides a hist function to create histograms."
      ],
      "metadata": {
        "id": "2SNfWDVOYzib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.random.randn(1000)\n",
        "# Create a histogram\n",
        "plt.hist(data, bins=30, color='blue', alpha=0.7)\n",
        "plt.title('Histogram')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Umn_mb2eY-c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BINNINGS"
      ],
      "metadata": {
        "id": "z-SJbV4RZLej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.random.randn(1000)\n",
        "bins = np.linspace(min(data), max(data), 30)\n",
        "# Create a histogram with custom bins\n",
        "plt.hist(data, bins=bins, color='green', alpha=0.7)\n",
        "plt.title('Binned Histogram')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "48enQgbZZNIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DENSITY"
      ],
      "metadata": {
        "id": "6UF7Tj-GZVOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Cqu2G-NIZifo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.random.randn(1000)\n",
        "# Create a density plot\n",
        "sns.kdeplot(data, shade=True, color='purple')\n",
        "plt.title('Density Plot')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yd2_cmlOZU1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART-C"
      ],
      "metadata": {
        "id": "1V-iSdzBo9mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform an exploratory data analysis  for the following data with different types ofplot"
      ],
      "metadata": {
        "id": "AvXtbwiJo_e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataset contains cases from a study that was conducted between 1958 and 1970 at the university of chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer"
      ],
      "metadata": {
        "id": "m034EpGwzrc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('breast_cancer_data.csv')"
      ],
      "metadata": {
        "id": "_iRXqoT7xT-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())"
      ],
      "metadata": {
        "id": "_rGmEUATzyjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "cG1VF1fcz6sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.describe())"
      ],
      "metadata": {
        "id": "iA9-hMSlz7VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AGE DISTRIBUTION"
      ],
      "metadata": {
        "id": "TX628ypTJerM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data['Age'])\n",
        "plt.title('Distribution of Age')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ma3hU_il0F8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YEAR OF OPERATION"
      ],
      "metadata": {
        "id": "nMK1erHbJmIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data['Year of Operation'])\n",
        "plt.title('Distribution of Year of Operation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eNKFkK8lJFpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POSITIVE  AXILLARY NODES"
      ],
      "metadata": {
        "id": "HNlgPFx3JqaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data['Positive Axillary Nodes'])\n",
        "plt.title('Distribution of Positive Axillary Nodes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pcClbEnbJR7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SURVIVAL STATUS"
      ],
      "metadata": {
        "id": "HgqWkkaJJzpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data['Survival Status'])\n",
        "plt.title('Survival Status Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-5VSVqSJZsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16)b)Assume that an r of -.80 describes the strong negative relationship  between years of heavy smoking(X) and life expectancy (Y)"
      ],
      "metadata": {
        "id": "jz0Cqj7IJ7rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume furthermore,that  the Distribution  of heavy smoking  and life expectancy  each have the following  means and sums of squares :5 60 35  70 x y X Y SS SS"
      ],
      "metadata": {
        "id": "Zw-OVo70VTK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Determine  the least square regression  equation for predicting life exceptancy from years of heavy smoking"
      ],
      "metadata": {
        "id": "GmpTn5UtVirA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_X = 5\n",
        "mean_Y = 60\n",
        "SS_X = 35\n",
        "SS_Y = 70\n",
        "r = -0.80\n",
        "# Calculate standard deviations\n",
        "SD_X = np.sqrt(SS_X)\n",
        "SD_Y = np.sqrt(SS_Y)\n",
        "# Calculate the slope (beta_1)\n",
        "beta_1 = r * (SD_Y / SD_X)\n",
        "# Calculate the intercept (beta_0)\n",
        "beta_0 = mean_Y - beta_1 * mean_X\n",
        "print(f\"Least Squares Regression Equation: Y = {beta_0:.2f} + {beta_1:.2f} * X\")"
      ],
      "metadata": {
        "id": "xEwkticcJ-oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Determine  the standard error of estimate ,Sy/x,assuming that the correlation of -80was based on n=50 pairs of Observations"
      ],
      "metadata": {
        "id": "c6sOE-1RXcPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = -0.80\n",
        "n = 50\n",
        "SS_Y = 70\n",
        "# Calculate standard error of estimate (Sy|x)\n",
        "Sy_x = np.sqrt((1 - r**2) * SS_Y / (n - 2))\n",
        "print(f\"Standard Error of the Estimate (Sy|x): {Sy_x:.2f}\")"
      ],
      "metadata": {
        "id": "ygvrg1w6XZUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)Supply a rough interpretation  of Sy/x"
      ],
      "metadata": {
        "id": "XPPSm0kJZGbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:The standard error of the estimate (��∣�Sy∣x​) gives an indication of how well the regression equation predicts the dependent variable (�Y) based on the independent variable (�X). In this context, ��∣�Sy∣x​ would represent the average amount by which actual life expectancies (�Y) deviate from the predicted values based on years of heavy smoking (�X)."
      ],
      "metadata": {
        "id": "OHnIT-yOZqzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)predict the life expectancy  for John,who has smoked heavily for 8years"
      ],
      "metadata": {
        "id": "68hk5vuFZfaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_john = 8\n",
        "Y_john = beta_0 + beta_1 * X_john\n",
        "print(f\"Predicted life expectancy for John: {Y_john:.2f} years\")"
      ],
      "metadata": {
        "id": "x25pQasmZ0cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)Predict  the life expectancy  for katie ,who has never smoked heavily"
      ],
      "metadata": {
        "id": "zJNMgQ7EbJ40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_katie = 0\n",
        "Y_katie = beta_0 + beta_1 * X_katie\n",
        "print(f\"Predicted life expectancy for Katie: {Y_katie:.2f} years\")"
      ],
      "metadata": {
        "id": "nEZUiNAebp1Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}